{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334b25d1-5c4e-479d-8da3-0c33bbbc6695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 13:25:46.258317: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-07 13:25:48.172323: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731003948.801076  915851 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731003949.026590  915851 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-07 13:25:51.476636: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, default_data_collator, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "import evaluate\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aabba8f-1a6f-4310-8b44-412ab054dbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7d535b59cb441ba20d514d3039fcc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a453510b77c476ba603dc1df1864b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c748df4688934f3089ae75c70092b209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_WER44 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e330b9f9e7b548869862d077e58d5510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_WER54 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SpokenSQuAD data completed\n"
     ]
    }
   ],
   "source": [
    "# Reformat a JSON file and save the result to a new file\n",
    "def modify_and_save_json(input_json):\n",
    "    try:\n",
    "        with open(input_json, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "    except IOError as e:\n",
    "        print(f\"Error opening {input_json}: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from {input_json}: {e}\")\n",
    "        return None\n",
    "\n",
    "    examples = []\n",
    "    for item in json_data['data']:\n",
    "        title = item['title'].strip()\n",
    "        for paragraph in item['paragraphs']:\n",
    "            context = paragraph['context'].strip()\n",
    "            for qa in paragraph['qas']:\n",
    "                example = {'id': qa['id'], 'title': title, 'context': context, 'question': qa['question'].strip(), 'answers': {'answer_start': [answer[\"answer_start\"] for answer in qa['answers']], 'text': [answer[\"text\"] for answer in qa['answers']]}}\n",
    "                examples.append(example)\n",
    "    \n",
    "    output_data = {'data': examples}\n",
    "    output_file = os.path.join(os.path.dirname(input_json), 'modified_' + os.path.basename(input_json))\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(output_data, f)\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to {output_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "    return output_file\n",
    "\n",
    "data_paths = {\n",
    "    'train': '/home/tvaspar/DL_HW3/spoken_train-v1.1.json',\n",
    "    'validation': '/home/tvaspar/DL_HW3/spoken_test-v1.1.json',\n",
    "    'test_WER44': '/home/tvaspar/DL_HW3/spoken_test-v1.1_WER44.json',\n",
    "    'test_WER54': '/home/tvaspar/DL_HW3/spoken_test-v1.1_WER54.json'\n",
    "}\n",
    "\n",
    "# Iterate over the paths of the original data files using a dictionary comprehension\n",
    "# and pass each file path to the modify_and_save_json function.\n",
    "# This will return a new dictionary containing the paths of the processed files.\n",
    "modified_data_files = {key: modify_and_save_json(path) for key, path in data_paths.items() if modify_and_save_json(path)}\n",
    "\n",
    "# Now, modified_data_files contains the paths of the processed files.\n",
    "# Use these paths to load the dataset.\n",
    "spoken_squad_dataset = load_dataset('json', data_files=modified_data_files, field='data')\n",
    "print(\"Loading SpokenSQuAD data completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "638fc1ac-7071-4020-9d8a-b137ba5381a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the BERT model and tokenizer from checkpoint 'bert-base-uncased'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'bert-base-uncased' successfully loaded for Question Answering tasks.\n",
      "Tokenizer for model 'bert-base-uncased' successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "checkpoint_model = \"bert-base-uncased\" \n",
    "print(f\"Loading the BERT model and tokenizer from checkpoint '{checkpoint_model}'...\") \n",
    "\n",
    "# Load the pre-trained Question Answering model from the specified checkpoint.\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint_model)\n",
    "print(f\"Model '{checkpoint_model}' successfully loaded for Question Answering tasks.\")\n",
    "\n",
    "# Load the tokenizer associated with the specified checkpoint.\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_model)\n",
    "print(f\"Tokenizer for model '{checkpoint_model}' successfully loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd38530-9878-4149-aa6d-752ad5656244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing of training data with tokenization and extraction of answer positions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135ae1b210d74fbbabce00d1c46463aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization and preprocessing of validation dataset (clean data, 22.73% WER) underway...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1c922ccfa4465c86d623a514f12c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test dataset with moderate noise level (44.22% WER) for evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7a1b912e914458929373b065cb8d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test dataset with high noise level (54.82% WER) for robustness assessment...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb81ebf8332a40be9bb2fbbf91ecd194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define data preprocessing functions\n",
    "max_length = 384  # Maximum length of the tokenized input sequences\n",
    "stride = 64  # The stride size for splitting long documents into chunks\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [question.strip() for question in examples['question']]\n",
    "    tokenized_inputs = tokenizer(\n",
    "        questions, \n",
    "        examples['context'],\n",
    "        max_length = max_length,\n",
    "        truncation = 'only_second',\n",
    "        stride = stride, \n",
    "        return_overflowing_tokens = True,\n",
    "        return_offsets_mapping=True, \n",
    "        padding = 'max_length'\n",
    "    )\n",
    "\n",
    "    offset_mapping = tokenized_inputs.pop('offset_mapping')\n",
    "    sample_mapping = tokenized_inputs.pop('overflow_to_sample_mapping')\n",
    "    answers = examples['answers']\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_mapping[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer['answer_start'][0]\n",
    "        end_char = answer['answer_start'][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = tokenized_inputs.sequence_ids(i)\n",
    "\n",
    "        # find start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1: \n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # if answer not fully inside context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "    \n",
    "    tokenized_inputs['start_positions'] = start_positions\n",
    "    tokenized_inputs['end_positions'] = end_positions\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "def process_validation_examples(examples):\n",
    "    questions = [question.strip() for question in examples['question']]\n",
    "    tokenized_inputs = tokenizer(\n",
    "        questions, \n",
    "        examples['context'],\n",
    "        max_length = max_length,\n",
    "        truncation = 'only_second',\n",
    "        stride = stride, \n",
    "        return_overflowing_tokens = True,\n",
    "        return_offsets_mapping=True, \n",
    "        padding = 'max_length'\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized_inputs.pop('overflow_to_sample_mapping')\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(tokenized_inputs['input_ids'])):\n",
    "        sample_idx = sample_mapping[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = tokenized_inputs.sequence_ids(i)\n",
    "        offsets = tokenized_inputs['offset_mapping'][i]\n",
    "        tokenized_inputs[\"offset_mapping\"][i] = [\n",
    "            offset if sequence_ids[k] == 1 else None for k, offset in enumerate(offsets)\n",
    "        ]\n",
    "\n",
    "    tokenized_inputs['example_id'] = example_ids\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "print(\"Starting preprocessing of training data with tokenization and extraction of answer positions...\")\n",
    "\n",
    "train_dataset = spoken_squad_dataset['train'].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=spoken_squad_dataset['train'].column_names\n",
    ")\n",
    "\n",
    "print(\"Tokenization and preprocessing of validation dataset (clean data, 22.73% WER) underway...\")\n",
    "\n",
    "validation_dataset = spoken_squad_dataset['validation'].map(\n",
    "    process_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=spoken_squad_dataset['validation'].column_names\n",
    ")\n",
    "\n",
    "print(\"Preprocessing test dataset with moderate noise level (44.22% WER) for evaluation...\")\n",
    "\n",
    "test_WER44_dataset = spoken_squad_dataset['test_WER44'].map(\n",
    "    process_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=spoken_squad_dataset['test_WER44'].column_names\n",
    ")\n",
    "\n",
    "print(\"Preprocessing test dataset with high noise level (54.82% WER) for robustness assessment...\")\n",
    "\n",
    "test_WER54_dataset = spoken_squad_dataset['test_WER54'].map(\n",
    "    process_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=spoken_squad_dataset['test_WER54'].column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6086b597-2846-4d91-9844-04770039436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted the training dataset to PyTorch tensor format.\n",
      "Prepared the validation dataset by removing 'example_id' and 'offset_mapping' columns and converting to PyTorch tensor format.\n",
      "Prepared the Test WER44 dataset (simulating 44% Word Error Rate) by removing unnecessary columns and converting to PyTorch tensor format.\n",
      "Prepared the Test WER54 dataset (simulating 54% Word Error Rate) by removing unnecessary columns and converting to PyTorch tensor format.\n",
      "Initializing the DataLoader for the training dataset with shuffling and a batch size of 8 to ensure varied mini-batch combinations during training.\n",
      "Initializing the DataLoader for the validation dataset with a batch size of 8 for model performance evaluation on unseen clean data.\n",
      "Initializing the DataLoader for the Test WER44 dataset with a batch size of 8 to evaluate model robustness under moderate noise conditions.\n",
      "Initializing the DataLoader for the Test WER54 dataset with a batch size of 8 to evaluate model robustness under high noise conditions.\n"
     ]
    }
   ],
   "source": [
    "# Convert the datasets to a format compatible with PyTorch models.\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "print(\"Converted the training dataset to PyTorch tensor format.\")\n",
    "\n",
    "validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "validation_set.set_format(\"torch\")\n",
    "print(\"Prepared the validation dataset by removing 'example_id' and 'offset_mapping' columns and converting to PyTorch tensor format.\")\n",
    "\n",
    "test_WER44_set = test_WER44_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "test_WER44_set.set_format(\"torch\")\n",
    "print(\"Prepared the Test WER44 dataset (simulating 44% Word Error Rate) by removing unnecessary columns and converting to PyTorch tensor format.\")\n",
    "\n",
    "test_WER54_set = test_WER54_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "test_WER54_set.set_format(\"torch\")\n",
    "print(\"Prepared the Test WER54 dataset (simulating 54% Word Error Rate) by removing unnecessary columns and converting to PyTorch tensor format.\")\n",
    "\n",
    "print(\"Initializing the DataLoader for the training dataset with shuffling and a batch size of 8 to ensure varied mini-batch combinations during training.\")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle = True, \n",
    "    collate_fn=default_data_collator, \n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(\"Initializing the DataLoader for the validation dataset with a batch size of 8 for model performance evaluation on unseen clean data.\")\n",
    "eval_loader = DataLoader(\n",
    "    validation_set, collate_fn=default_data_collator, batch_size=8\n",
    ")\n",
    "print(\"Initializing the DataLoader for the Test WER44 dataset with a batch size of 8 to evaluate model robustness under moderate noise conditions.\")\n",
    "test_WER44_loader= DataLoader(\n",
    "    test_WER44_set, collate_fn=default_data_collator, batch_size=8\n",
    ")\n",
    "print(\"Initializing the DataLoader for the Test WER54 dataset with a batch size of 8 to evaluate model robustness under high noise conditions.\")\n",
    "test_WER54_loader = DataLoader(\n",
    "    test_WER54_set, collate_fn=default_data_collator, batch_size=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4984a36f-4387-436c-8ecb-9686129c2beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics and evaluation function\n",
    "metric = evaluate.load(\"squad\")  # Load the SQuAD evaluation metric\n",
    "\n",
    "n_best = 20  # Number of top predictions to consider for each example\n",
    "max_answer_length = 30  # Maximum length of an answer that can be generated\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)  # Map each example_id to its corresponding features\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "    \n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):  # Iterate through each example\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "        \n",
    "        # Loop through all features associated with an example ID\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]  # Start logit scores for this feature\n",
    "            end_logit = end_logits[feature_index]  # End logit scores for this feature\n",
    "            offsets = features[feature_index][\"offset_mapping\"]  # Token offsets for this feature\n",
    "            \n",
    "            # Get indices of the n_best start and end logits\n",
    "            start_indexes = np.argsort(start_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully within the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with invalid lengths\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    \n",
    "                    # Construct an answer candidate\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0]: offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index]\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "        \n",
    "        # Select the answer with the highest logit score\n",
    "        if answers:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "    \n",
    "    # Compare predicted answers with the actual answers\n",
    "    theory_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theory_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27604671-eeb1-489b-a7b1-0d2178942632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model=model, train_loader=train_loader, eval_loader=eval_loader, epochs=3):\n",
    "    print(f\"Starting model training for {epochs} epochs, each with {len(train_loader)} batches.\")\n",
    "    \n",
    "    training_steps = epochs * len(train_loader)  # Total training steps calculation\n",
    "\n",
    "    # Initialize the Accelerator for mixed precision training\n",
    "    accelerator = Accelerator(mixed_precision='fp16')\n",
    "    print(\"Accelerator initialized for mixed precision ('fp16') training.\")\n",
    "\n",
    "    # Set up the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    print(\"Optimizer setup with learning rate 2e-5.\")\n",
    "\n",
    "    # Prepare model, optimizer, and dataloaders for Accelerator\n",
    "    model, optimizer, train_loader, eval_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, eval_loader\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler initialization\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps,\n",
    "    )\n",
    "\n",
    "    progress_bar = tqdm(range(training_steps), desc=\"Training Progress\")  # Training progress bar setup\n",
    "\n",
    "    for epoch in range(epochs):  # Loop over epochs\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} - Training:\")\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        total_loss = 0  # Initialize total loss for averaging\n",
    "        correct_predictions = 0  # Initialize correct predictions counter\n",
    "\n",
    "        for step, batch in enumerate(train_loader):  # Iterate over training batches\n",
    "            outputs = model(**batch)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()  # Accumulate loss\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # Model evaluation at the end of each epoch\n",
    "        print(\"\\nEvaluating model performance on the validation set...\")\n",
    "        metrics = evaluate_model(model, eval_loader, validation_dataset, spoken_squad_dataset['validation'], accelerator)\n",
    "        \n",
    "        # Calculate and print average training loss for the epoch\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} - Average Training Loss: {average_loss:.4f}\")\n",
    "\n",
    "        # Print validation metrics for accuracy assessment\n",
    "        print(f\"Validation Results - Epoch {epoch+1}: {metrics}\")\n",
    "\n",
    "        # Save the model and tokenizer at the end of each epoch\n",
    "        output_dir = f\"./model_save/epoch_{epoch+1}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        model_to_save = accelerator.unwrap_model(model)\n",
    "        model_to_save.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Model and tokenizer saved in '{output_dir}'\")\n",
    "\n",
    "    print(\"\\nTraining completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f97593f-5d6c-4da4-92af-737a33368f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating model fine-tuning process...\n",
      "Launching training on one GPU.\n",
      "Starting model training for 3 epochs, each with 4664 batches.\n",
      "Accelerator initialized for mixed precision ('fp16') training.\n",
      "Optimizer setup with learning rate 2e-5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d3e9789006458e8fe5c29e597d07b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/13992 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3 - Training:\n",
      "\n",
      "Evaluating model performance on the validation set...\n",
      "Setting the model to evaluation mode for performance assessment...\n",
      "Evaluating model performance on the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c376d6367ff747d2aaa9539ba0685926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Progress:   0%|          | 0/678 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing evaluation metrics based on model predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e04661e5e94555a0e0d83f17819004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Training Loss: 1.7089\n",
      "Validation Results - Epoch 1: {'exact_match': 61.48383479723416, 'f1': 72.1856383081878}\n",
      "Model and tokenizer saved in './model_save/epoch_1'\n",
      "\n",
      "Epoch 2/3 - Training:\n",
      "\n",
      "Evaluating model performance on the validation set...\n",
      "Setting the model to evaluation mode for performance assessment...\n",
      "Evaluating model performance on the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342bb23aa2e74405b9fe041dfd58f600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Progress:   0%|          | 0/678 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing evaluation metrics based on model predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b8377f2b064be5a0baf4fe55f9fad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average Training Loss: 0.9422\n",
      "Validation Results - Epoch 2: {'exact_match': 63.7077181835171, 'f1': 73.93310041695544}\n",
      "Model and tokenizer saved in './model_save/epoch_2'\n",
      "\n",
      "Epoch 3/3 - Training:\n",
      "\n",
      "Evaluating model performance on the validation set...\n",
      "Setting the model to evaluation mode for performance assessment...\n",
      "Evaluating model performance on the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f8fa58000c4203be9d30ca84d372c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Progress:   0%|          | 0/678 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing evaluation metrics based on model predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df3f837e36049e9a0e069f35a03af28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Average Training Loss: 0.5974\n",
      "Validation Results - Epoch 3: {'exact_match': 63.68903008783405, 'f1': 74.11362060254861}\n",
      "Model and tokenizer saved in './model_save/epoch_3'\n",
      "\n",
      "Training completed successfully.\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader, dataset, dataset_before_preprocessing, accelerator=None):\n",
    "    if not accelerator:\n",
    "        print(\"Initializing Accelerator for mixed precision (fp16) evaluation...\")\n",
    "        accelerator = Accelerator(mixed_precision='fp16')\n",
    "        model, dataloader = accelerator.prepare(model, dataloader)\n",
    "    \n",
    "    print(\"Setting the model to evaluation mode for performance assessment...\")\n",
    "    model.eval()\n",
    "    start_logits, end_logits = [], []\n",
    "\n",
    "    print(\"Evaluating model performance on the dataset...\")\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluation Progress\"):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
    "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
    "\n",
    "    # Concatenate and truncate logits to align with the dataset size\n",
    "    start_logits, end_logits = np.concatenate(start_logits)[:len(dataset)], np.concatenate(end_logits)[:len(dataset)]\n",
    "\n",
    "    print(\"Computing evaluation metrics based on model predictions...\")\n",
    "    metrics = compute_metrics(start_logits, end_logits, dataset, dataset_before_preprocessing)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Initiating model fine-tuning process...\")\n",
    "notebook_launcher(train_model, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7a13bc8-b50d-444b-af6a-18c252427755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Accelerator for mixed precision (fp16) evaluation...\n",
      "Setting the model to evaluation mode for performance assessment...\n",
      "Evaluating model performance on the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c702a1fdd4b45eb89859f0a71197931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Progress:   0%|          | 0/678 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing evaluation metrics based on model predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a382b20a5c384100a9371125daa9486d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Accelerator for mixed precision (fp16) evaluation...\n",
      "Setting the model to evaluation mode for performance assessment...\n",
      "Evaluating model performance on the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9988b5c2fbaf451f90068758d3b7fb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Progress:   0%|          | 0/679 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing evaluation metrics based on model predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f3834b50bb4343bf4547d7424c8da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Accelerator for mixed precision (fp16) evaluation...\n",
      "Setting the model to evaluation mode for performance assessment...\n",
      "Evaluating model performance on the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df8b9aca318474bbe3fa909ec692fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Progress:   0%|          | 0/679 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing evaluation metrics based on model predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4ec3fe34bd4db19ab50de8ab42e097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation Summary\n",
      "\n",
      "Validation Set:\n",
      "  - F1 Score: 74.11%\n",
      "\n",
      "Test WER44 Set:\n",
      "  - F1 Score: 55.77%\n",
      "\n",
      "Test WER54 Set:\n",
      "  - F1 Score: 42.29%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results Evaluation\n",
    "\n",
    "test_metrics = evaluate_model(model, eval_loader, validation_dataset, spoken_squad_dataset['validation'])\n",
    "test_wer44_metrics = evaluate_model(model, test_WER44_loader, test_WER44_dataset, spoken_squad_dataset['test_WER44'])\n",
    "test_wer54_metrics = evaluate_model(model, test_WER54_loader, test_WER54_dataset, spoken_squad_dataset['test_WER54'])\n",
    "\n",
    "# Model Evaluation Summary\n",
    "print(\"\\nModel Evaluation Summary\\n\")\n",
    "\n",
    "# Validation Set results\n",
    "print(\"Validation Set:\")\n",
    "print(f\"  - F1 Score: {test_metrics['f1']:.2f}%\\n\")\n",
    "\n",
    "# Test WER44 Set results\n",
    "print(\"Test WER44 Set:\")\n",
    "print(f\"  - F1 Score: {test_wer44_metrics['f1']:.2f}%\\n\")\n",
    "\n",
    "# Test WER54 Set results\n",
    "print(\"Test WER54 Set:\")\n",
    "print(f\"  - F1 Score: {test_wer54_metrics['f1']:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d453f957-a599-4fa4-9056-f5cbd6933bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizerFast, AdamW\n",
    "# AutoTokenizer, AutoModelForQuestionAnswering, BertTokenizer, BertForQuestionAnswering\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ab56944-e73d-48b1-8cba-01c7ed9354de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "ques_num = 0\n",
    "pos_num = 0\n",
    "impos_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a70f1832-d858-431b-8393-d36f54bd0a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_data(path): \n",
    "    #read each file and retrieve the contexts, qustions and answers\n",
    "    with open(path, 'rb') as f:\n",
    "        raw_data = json.load(f)\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    num_q = 0\n",
    "    num_pos = 0\n",
    "    num_imp = 0\n",
    "\n",
    "    for group in raw_data['data']:\n",
    "        for paragraph in group['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question= qa['question']\n",
    "                num_q  = num_q  +1\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context.lower())\n",
    "                    questions.append(question.lower())\n",
    "                    answers.append(answer)\n",
    "    return num_q, num_pos, num_imp, contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "233e8a73-a3b2-4a9a-8e74-539fbae3d449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_q, num_pos, num_imp, train_contexts, train_questions, train_answers = get_data('spoken_train-v1.1.json')\n",
    "ques_num  = num_q\n",
    "pos_num = num_pos\n",
    "impos_num  = num_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "652b4818-74bd-40cc-8094-ee9cce18732d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_q, num_pos, num_imp, valid_contexts, valid_questions, valid_answers = get_data('spoken_test-v1.1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b268f87-6577-46b6-975b-9d800affcc46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_answer_end(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        answer['text'] = answer['text'].lower()\n",
    "        answer['answer_end'] = answer['answer_start'] + len(answer['text'])\n",
    "\n",
    "update_answer_end(train_answers, train_contexts)\n",
    "update_answer_end(valid_answers, valid_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fea68066-6e15-407c-9a27-f746a279e593",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37111\n",
      "37111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 512\n",
    "PRETRAINED_MODEL= \"bert-base-uncased\"\n",
    "\n",
    "doc_stride = 128\n",
    "tokenizerFast = BertTokenizerFast.from_pretrained(PRETRAINED_MODEL)\n",
    "is_padding_right = tokenizerFast.padding_side == \"right\"\n",
    "train_contexts_truncated=[]\n",
    "print(len(train_contexts))\n",
    "for i in range(len(train_contexts)):\n",
    "    if(len(train_contexts[i])>512):\n",
    "        answer_start=train_answers[i]['answer_start']\n",
    "        answer_end=train_answers[i]['answer_start']+len(train_answers[i]['text'])\n",
    "        mid=(answer_start+answer_end)//2\n",
    "        para_start=max(0,min(mid - MAX_LENGTH//2,len(train_contexts[i])-MAX_LENGTH))\n",
    "        para_end = para_start + MAX_LENGTH \n",
    "        train_contexts_truncated.append(train_contexts[i][para_start:para_end])\n",
    "        train_answers[i]['answer_start']=((512/2)-len(train_answers[i])//2)\n",
    "    else:\n",
    "        train_contexts_truncated.append(train_contexts[i])  \n",
    "    \n",
    "print(len(train_contexts_truncated))\n",
    "train_encodings_fast = tokenizerFast(train_questions, train_contexts_truncated,  max_length = MAX_LENGTH,truncation=True,\n",
    "        stride=doc_stride,\n",
    "        padding=True)\n",
    "valid_encodings_fast = tokenizerFast(valid_questions,valid_contexts,  max_length = MAX_LENGTH, truncation=True,stride=doc_stride,\n",
    "        padding=True)\n",
    "type(train_encodings_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b9271e2-a098-4b3f-8728-900da5b22c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_answer_start_and_end_train(idx):\n",
    "    start_pos = 0\n",
    "    end_pos = 0\n",
    "    answer_encoding = tokenizerFast(train_answers[idx]['text'],  max_length = MAX_LENGTH, truncation=True, padding=True)\n",
    "    for a in range( len(train_encodings_fast['input_ids'][idx]) -  len(answer_encoding['input_ids']) ): #len(train_encodings_fast['input_ids'][0])):\n",
    "        match = True\n",
    "        for i in range(1,len(answer_encoding['input_ids']) - 1):\n",
    "            if (answer_encoding['input_ids'][i] != train_encodings_fast['input_ids'][idx][a + i]):\n",
    "                match = False\n",
    "                break\n",
    "            if match:\n",
    "                start_pos = a+1\n",
    "                end_pos = a+i+1\n",
    "                break\n",
    "    return(start_pos, end_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b063f48b-9ecb-485b-aa46-fdc4744a246a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "ctr = 0\n",
    "for h in range(len(train_encodings_fast['input_ids'])):\n",
    "    s, e = get_answer_start_and_end_train(h)\n",
    "    start_positions.append(s)\n",
    "    end_positions.append(e)\n",
    "    if s==0:\n",
    "        ctr = ctr + 1\n",
    "    \n",
    "train_encodings_fast.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "print(ctr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54a3a104-4d39-40a8-b79a-4813bb364b49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_answer_start_and_end_valid(idx):\n",
    "    start_pos = 0\n",
    "    end_pos = 0\n",
    "    answer_encoding = tokenizerFast(valid_answers[idx]['text'],  max_length = MAX_LENGTH, truncation=True, padding=True)\n",
    "    for a in range( len(valid_encodings_fast['input_ids'][idx])  -  len(answer_encoding['input_ids'])   ): #len(train_encodings_fast['input_ids'][0])):\n",
    "        match = True\n",
    "        for i in range(1,len(answer_encoding['input_ids']) - 1):\n",
    "            if (answer_encoding['input_ids'][i] != valid_encodings_fast['input_ids'][idx][a + i]):\n",
    "                match = False\n",
    "                break\n",
    "            if match:\n",
    "                start_pos = a+1\n",
    "                end_pos = a+i+1\n",
    "                break\n",
    "    return(start_pos, end_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ba1101d-79e7-4240-adf1-75ded0a5d47e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n"
     ]
    }
   ],
   "source": [
    "start_positions = []\n",
    "end_positions = []\n",
    "ctr = 0\n",
    "for h in range(len(valid_encodings_fast['input_ids']) ):\n",
    "    #print(h)\n",
    "    s, e = get_answer_start_and_end_valid(h)\n",
    "    start_positions.append(s)\n",
    "    end_positions.append(e)\n",
    "    if s==0:\n",
    "        ctr = ctr + 1\n",
    "\n",
    "valid_encodings_fast.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "print(ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96154753-58e2-4b4a-b489-cc464f899361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][i]),\n",
    "            'token_type_ids': torch.tensor(self.encodings['token_type_ids'][i]),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][i]),\n",
    "            'start_positions': torch.tensor(self.encodings['start_positions'][i]),\n",
    "            'end_positions': torch.tensor(self.encodings['end_positions'][i])\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings_fast)\n",
    "valid_dataset = CustomDataset(valid_encodings_fast)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=1)\n",
    "\n",
    "bert_model = BertModel.from_pretrained(PRETRAINED_MODEL)  #PRETRAINED_MODEL = \"bert-base-uncased\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5c12da3-f12c-41b4-904d-2671d060d1b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class QuestionAnsweringModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuestionAnsweringModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.l1 = nn.Linear(768 * 2, 768 * 2)\n",
    "        self.l2 = nn.Linear(768 * 2, 2)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            self.drop_out,\n",
    "            self.l1,\n",
    "            nn.LeakyReLU(),\n",
    "            self.l2 \n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        bert_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)\n",
    "        hidden_state_layers = bert_output[2]\n",
    "        out = torch.cat((hidden_state_layers[-1], hidden_state_layers[-3]), dim=-1)  # taking Start logits from last BERT layer, End Logits from third to last layer\n",
    "        logits = self.linear_relu_stack(out)\n",
    "        \n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        \n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9698391-5d4d-4657-a916-cbf64cad11f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = QuestionAnsweringModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba57b733-ccec-4328-9ea6-9cbb1e89ba23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tvaspar/.local/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# my function to manually calculate Cross Entropy Loss\n",
    "def custom_loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss_value= loss_fct(start_logits, start_positions)\n",
    "    end_loss_value = loss_fct(end_logits, end_positions)\n",
    "    total_loss = (start_loss_value + end_loss_value)/2\n",
    "    return total_loss\n",
    "\n",
    "def custom_focal_loss_fn(start_logits, end_logits, start_positions, end_positions, gamma_value):\n",
    "    \n",
    "    #calculate Probabilities by applying Softmax to the Start and End Logits. Then get 1 - probabilities\n",
    "    softmax_func = nn.Softmax(dim=1)\n",
    "    start_probs = softmax_func(start_logits)\n",
    "    inv_start_probs = 1 - start_probs\n",
    "    end_probs = softmax_func(end_logits)\n",
    "    inv_end_probs = 1 - end_probs\n",
    "    \n",
    "    #get log of probabilities. Note: NLLLoss required log probabilities. This is the Natural Log (Log base e)\n",
    "    log_softmax_func = nn.LogSoftmax(dim=1)\n",
    "    start_log_probs = log_softmax_func(start_logits)\n",
    "    end_log_probs = log_softmax_func(end_logits)\n",
    "    \n",
    "    nll = nn.NLLLoss()\n",
    "    \n",
    "    focal_start_loss = nll(torch.pow(inv_start_probs, gamma_value)* start_log_probs, start_positions)\n",
    "    focal_end_loss = nll(torch.pow(inv_end_probs, gamma_value)*end_log_probs, end_positions)\n",
    "    \n",
    "    #return mean of the Loss for the start and end logits\n",
    "    return ((focal_start_loss + focal_end_loss)/2)\n",
    "optim = AdamW(model.parameters(), lr=2e-5, weight_decay=2e-2)\n",
    "EPOCHS = 3\n",
    "total_steps=len(train_dataset)*EPOCHS\n",
    "scheduler=transformers.get_linear_schedule_with_warmup(optim,num_warmup_steps=0,num_training_steps=total_steps )\n",
    "total_acc = []\n",
    "total_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10d1fe3b-1246-4fe6-aeb8-83a164c96ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_epoch(model, dataloader, epoch):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    acc = []\n",
    "    ctr = 0\n",
    "    batch_counter = 0\n",
    "    for batch in tqdm(dataloader, desc = 'Running Epoch '):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        start_logits, end_logits = model(input_ids=input_ids, \n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "        #loss = custom_loss_fn(start_logits, end_logits, start_positions, end_positions)  # <---BASELINE.  Cross Entropy Loss is returned by Default\n",
    "        loss = custom_focal_loss_fn(start_logits, end_logits, start_positions, end_positions,1) #using gamma_value = 1\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        start_predictions = torch.argmax(start_logits, dim=1)\n",
    "        end_predictions = torch.argmax(end_logits, dim=1)\n",
    "            \n",
    "        acc.append(((start_predictions == start_positions).sum()/len(start_predictions)).item())\n",
    "        acc.append(((end_predictions == end_positions).sum()/len(end_predictions)).item())\n",
    "        #ctr = ctr +1\n",
    "        #if ctr==50:\n",
    "        #    break\n",
    "        batch_counter = batch_counter + 1\n",
    "        if batch_counter==250 and epoch==1:\n",
    "            total_acc.append(sum(acc))\n",
    "            loss_avg = sum(losses)/len(losses)\n",
    "            total_loss.append(loss_avg)\n",
    "            batch_counter = 0\n",
    "    scheduler.step()\n",
    "    average_accuracy = sum(acc)/len(acc)\n",
    "    average_loss = sum(losses)/len(losses)\n",
    "    return(average_accuracy, average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03d72038-ed5e-4260-92c0-dc01f749a963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Epoch : 100%|| 4639/4639 [04:49<00:00, 16.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.6905506112807319      Train Loss: 0.7980141290131251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Evaluation: 100%|| 15875/15875 [02:33<00:00, 103.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.4409448818897638, F1 Score: 0.49185593482532547\n",
      "Epoch 0: WER Score: 2.480515999329871, F1 Score: 0.49185593482532547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Epoch : 100%|| 4639/4639 [04:51<00:00, 15.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8036580390009188      Train Loss: 0.4405247646554185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Evaluation: 100%|| 15875/15875 [02:32<00:00, 103.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.43968503937007875, F1 Score: 0.48695535145640984\n",
      "Epoch 1: WER Score: 2.5994303903501423, F1 Score: 0.48695535145640984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Epoch : 100%|| 4639/4639 [04:51<00:00, 15.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8732620176762234      Train Loss: 0.25790816928034627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Evaluation: 100%|| 15875/15875 [02:31<00:00, 104.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.4324409448818898, F1 Score: 0.48302312366909295\n",
      "Epoch 2: WER Score: 2.160931479309767, F1 Score: 0.48302312366909295\n",
      "WER Scores: [2.480515999329871, 2.5994303903501423, 2.160931479309767]\n",
      "F1 Scores: [0.49185593482532547, 0.48695535145640984, 0.48302312366909295]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def eval_model(model, dataloader):\n",
    "    model = model.eval()\n",
    "    acc = []\n",
    "    f1_scores = []\n",
    "    answer_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Running Evaluation'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            start_true = batch['start_positions'].to(device)\n",
    "            end_true = batch['end_positions'].to(device)\n",
    "            \n",
    "            start_logits, end_logits = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            start_predictions = torch.argmax(start_logits, dim=1).item()  # Convert to scalar\n",
    "            end_predictions = torch.argmax(end_logits, dim=1).item()      # Convert to scalar\n",
    "            \n",
    "            # Ensure valid slicing by adjusting end_predictions if it's before start_predictions\n",
    "            if end_predictions < start_predictions:\n",
    "                end_predictions = start_predictions\n",
    "\n",
    "            # Collecting the answers\n",
    "            answer = tokenizerFast.convert_tokens_to_string(\n",
    "                tokenizerFast.convert_ids_to_tokens(input_ids[0][start_predictions:end_predictions + 1])\n",
    "            )\n",
    "            tanswer = tokenizerFast.convert_tokens_to_string(\n",
    "                tokenizerFast.convert_ids_to_tokens(input_ids[0][start_true[0].item():end_true[0].item() + 1])\n",
    "            )\n",
    "            answer_list.append([answer, tanswer])\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            acc.append((start_predictions == start_true[0].item()) and (end_predictions == end_true[0].item()))\n",
    "            \n",
    "            # Calculate F1 score for this batch\n",
    "            pred_span = set(range(start_predictions, end_predictions + 1))\n",
    "            true_span = set(range(start_true[0].item(), end_true[0].item() + 1))\n",
    "            overlap = len(pred_span & true_span)\n",
    "            \n",
    "            if len(pred_span) > 0 and len(true_span) > 0:\n",
    "                precision = overlap / len(pred_span)\n",
    "                recall = overlap / len(true_span)\n",
    "                if precision + recall > 0:\n",
    "                    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "                else:\n",
    "                    f1 = 0\n",
    "            else:\n",
    "                f1 = 0\n",
    "\n",
    "            f1_scores.append(f1)\n",
    "    \n",
    "    average_accuracy = sum(acc) / len(acc)\n",
    "    average_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    print(f\"Evaluation Accuracy: {average_accuracy}, F1 Score: {average_f1}\")\n",
    "    \n",
    "    return answer_list, average_f1\n",
    "\n",
    "# Main training loop with F1 score printing\n",
    "wer_list = []\n",
    "f1_list = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_acc, train_loss = run_epoch(model, train_data_loader, epoch + 1)\n",
    "    print(f\"Train Accuracy: {train_acc}      Train Loss: {train_loss}\")\n",
    "    \n",
    "    answer_list, avg_f1 = eval_model(model, valid_data_loader)\n",
    "    \n",
    "    pred_answers = []\n",
    "    true_answers = []\n",
    "    for i in range(len(answer_list)):\n",
    "        if len(answer_list[i][0]) == 0:\n",
    "            answer_list[i][0] = \"$\"\n",
    "        if len(answer_list[i][1]) == 0:\n",
    "            answer_list[i][1] = \"$\"\n",
    "        pred_answers.append(answer_list[i][0])\n",
    "        true_answers.append(answer_list[i][1])\n",
    "    \n",
    "    wer_score = wer.compute(predictions=pred_answers, references=true_answers)\n",
    "    print(f\"Epoch {epoch}: WER Score: {wer_score}, F1 Score: {avg_f1}\")\n",
    "    \n",
    "    wer_list.append(wer_score)\n",
    "    f1_list.append(avg_f1)\n",
    "\n",
    "# Save WER and F1 scores\n",
    "with open(\"base_model_metrics.txt\", 'w') as f:\n",
    "    for wer, f1 in zip(wer_list, f1_list):\n",
    "        f.write(f\"WER: {wer}, F1: {f1}\\n\")\n",
    "\n",
    "print(\"WER Scores:\", wer_list)\n",
    "print(\"F1 Scores:\", f1_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec1a9d-1342-444c-b3c8-79ff0fb6551e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
